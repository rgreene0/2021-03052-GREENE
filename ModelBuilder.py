# -*- coding: utf-8 -*-
"""Copy of GitDownloadItems

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AcnizqK5_vtCLnRz0dPtUc_yjMVJkzN4

# Imports and Setup
Global items which can be used for configuration

### Global Parameters
"""

# ---- pick a project ---
# provide a list of any repositories to be tracked as an array, e.g. [rgreene0/2021-03052-GREENE, ...]
allowedRepos = [, , , , ]
# provide a list of specific users to be tracked, leave blank for all
allowedUsers  = ''
# storage location 
root ='/content/DATA/'
# general params 
numberCommits = 4000
filter_by_weeks = 52 * 2  # two years
filter_weeks = 30         # default number of weeks to take for analysis 
fraud_sample = [0.70, 0.30]  # random selection of comments from contributors
filter_by_commit_count = 10 ## min number of commits allowed before being considered having enough data
hide_values = True
clean_up_after_export = False
shuffle_data=False
dataHeaders = ['id', 'date', 'year_month', 'hour_of_day', 'day_of_year',
               'day_of_week', 'files_changed', 'files_modified', 'files_added', 'files_deleted', 'files_renamed',
               'isWeekend', 'isMerge', 'words_in_comment', 'word_vectors', 'person']

"""## Global functions
This section contains the main functions used to generate data across all others
"""

import pandas as pd
pd.options.mode.chained_assignment = None
pd.options.display.max_columns = None
import getopt
import json
import os
import shutil
import subprocess
import hashlib
import seaborn as sns
from subprocess import check_output # for executing shell processes and returning output as string
import sys
from urllib.request import urlopen
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime as dt, timedelta
import warnings


#--- global functions
def oneWayEncrypt(text):
  if hide_values == False :
    return text
  unicode = text.encode('UTF-8')
  hash_object = hashlib.sha512(unicode)
  hex_dig = hash_object.hexdigest()
  return hex_dig[:40]

def angular_distance(time):
  sin = np.sin( time * (2 * np.pi /7))
  cos = np.cos( time * (2 * np.pi /7))
  print (sin, cos)
  return sin+cos

def add_time_vectors_fields(dataFrame):
  # Add additional column data
  hour_in_day = 24
  days_in_week = 7
  day_in_year = 365
  dataFrame['hour_of_day_sin'] = np.sin(2*np.pi*dataFrame['hour_of_day']/hour_in_day)
  dataFrame['hour_of_day_cos'] = np.cos(2*np.pi*dataFrame['hour_of_day']/hour_in_day)
  dataFrame['day_of_week_sin'] = np.sin(2*np.pi*dataFrame['day_of_week']/days_in_week)
  dataFrame['day_of_week_cos'] = np.cos(2*np.pi*dataFrame['day_of_week']/days_in_week)
  dataFrame['day_of_year_sin'] = np.sin(2*np.pi*dataFrame['day_of_year']/day_in_year)
  dataFrame['day_of_year_cos'] = np.cos(2*np.pi*dataFrame['day_of_year']/day_in_year)  
  return dataFrame

def readSourceDataFile(currentRepo):
  os.chdir(root)
  resultFile = "data_{0}.csv".format(currentRepo)
  data = pd.read_csv(resultFile, header=0, parse_dates=['date'])
  data = add_time_vectors_fields(data)
  ## exclude records based on critera
  # - Number of commits
  for name, group in data.groupby('person'):
    if(len(group) <= filter_by_commit_count):
      data = data[data.person != name]
  data.rename( columns={'Unnamed: 0':'id'}, inplace=True )
  return data

# input - df: a Dataframe, chunkSize: the chunk size
# output - a list of DataFrame
# purpose - splits the DataFrame into smaller chunks
def split_dataframe(df, chunk_ratio = .5): 
    chunks = list()
    chunk_size = round(len(df) * chunk_ratio)
    num_chunks = len(df) // chunk_size
    for i in range(num_chunks):
        chunks.append(df[i*chunk_size:(i+1)*chunk_size])
    return chunks

"""# Extract data files and Clean 
Use GitLog to extract the checking details from all specified repos.  Repos selected are noted in the global import and setup section.

See the following for formats: https://git-scm.com/docs/git-log

"""

if(not os.path.isdir(root)):
  os.mkdir(root)
os.chdir(root)

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  print(currentRepo)
  commitsLink = "https://api.github.com/repos/{0}/commits".format(allowedRepo)
  f = urlopen(commitsLink)
  commits = json.loads(f.readline())
  print("repo: '{0}' - {1}".format(currentRepo, len(commits)))

  # clone the latest version 
  repoLink = "https://github.com/{0}.git".format(allowedRepo)

  if not os.path.isdir(currentRepo):
    print("getting the project")
    subprocess.call(['git', 'clone', repoLink, currentRepo])

"""## Commits and changes
Get a list if the two types of code metadata; first is all commits, second are the specific chang details.
"""

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  # extract the data via GIT LOG
  resultFile = "{0}.csv".format(oneWayEncrypt(currentRepo))
  os.chdir(root)
  os.chdir(currentRepo)

  filter_date = dt.today() - timedelta(weeks=filter_by_weeks)
  # execute a git log command. 
  # nb some of the content may have internal " which stuffs the csv loading. 
  # so we start with ___ field delimiters, sub out any quotes, then replace quotes as field delimiters
  result = check_output(["git", "log", "--since", filter_date.strftime('%Y-%m-%d'), "--date=iso-strict", "--pretty=format:___%h___,___%an___,___%ad___,___%s___"]).decode("utf8").replace('"','*').replace('___','"')

  os.chdir(root)
  #remove existing file if needed
  if os.path.exists(resultFile):
      os.remove(resultFile)
  # write the details
  textfile = open(resultFile, "w")
  textfile.write(result)
  textfile.close()
  print("-- sample results --")
  with open(resultFile) as datafile:
      head = [next(datafile) for x in range(10)]
  print(head)

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  resultFile = "{0}_changes.csv".format(oneWayEncrypt(currentRepo))
  os.chdir(root)
  os.chdir(currentRepo)

  filter_date = dt.today() - timedelta(weeks=filter_by_weeks)
  result = check_output(["git", "log", "--since", filter_date.strftime('%Y-%m-%d'), "--name-status", "--no-decorate"]).decode("utf8")

  os.chdir(root)
  #remove existing file if needed
  if os.path.exists(resultFile):
      os.remove(resultFile)
  # write the details 
  textfile = open(resultFile, "w")
  textfile.write(result)
  textfile.close()
  print("-- sample results --")
  with open(resultFile) as datafile:
      head = [next(datafile) for x in range(10)]
  print(head)

"""## Create model data file from the raw data gathered and clean up any data issues.
Read the two source datafile and create a single results file which can be stored and used for graphing.
"""

import pytz
import re
import math
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

## get file details from data 
def readChangeDetails(commitID, currentRepo) :
  sourceFile = "{0}_changes.csv".format(oneWayEncrypt(currentRepo))
  commitFile = open(sourceFile, "r")
  inRecordingMode = False
  value = "";
  for line in commitFile:
    if not inRecordingMode:
      if line.startswith('commit '+ commitID):
        inRecordingMode = True
    elif line.startswith('commit ') and inRecordingMode:
      inRecordingMode = False
    else:
      value = value + line
  return value

def fileCount(text, letter) :
  count = 0
  for line in text.splitlines() :
    #ignore the standard stuff
    if(line.startswith("Author:") or line.startswith("Date:") or line.startswith("Merge:")) :
      continue
    ## check this value
    if line.startswith(letter + '\t'):
      count = count + 1
  return count

def isMergeRequest(text) :
  if "Merge pull request " in text :
    return 1
  if "Merge branch " in text :
    return 1    
  return 0

def extractTopLevelFolders(ids) :
  commitIds = pd.DataFrame(ids)
  #first build a dataframe with the folders
  sourceFile = "{0}_changes.csv".format(oneWayEncrypt(currentRepo))
  commitFile = open(sourceFile, "r")
  dirs=[]
  for line in commitFile:
    if line.startswith(tuple(['A\t', 'M\t', 'D\t', 'R\t'])):
      dirName = line.split('/')
      if(not (dirName[0].find('.') > 0 or dirName[0] == "")):
        dirs.append(dirName[0][2:].replace('\n',''))
  toplevelFolders = pd.DataFrame(dirs)
  for col in toplevelFolders[0].unique():
    ids[col] = 0
  
  #Now for each commit we check the files that changed and update
  #the highlevel directory
  for index, row in df.iterrows():
    dirs = []
    update = readChangeDetails(row["commit"], currentRepo) 
    for line in update.splitlines():
      if line.startswith(tuple(['A\t', 'M\t', 'D\t', 'R\t'])):
        dirName = line.split('/')
        dirName = "dir_" + dirName[0].replace('\n','')[2:]
        if(not (dirName.find('.') > -1 or dirName == "") ):
          if(not dirName in dirs) :
            dirs.append(dirName)
  ##dirs now has all the folders hit by the commits so we set them to 1
    for dir in dirs :
      commitIds.at[index,dir]=1
  return commitIds.set_index('commit')

def extractFileTypes(ids) :
  commitIds = pd.DataFrame(ids)
  #first build a dataframe with the folders
  sourceFile = "{0}_changes.csv".format(oneWayEncrypt(currentRepo))
  commitFile = open(sourceFile, "r")
  exts=[]
  for line in commitFile:
    if line.startswith(tuple(['A\t', 'M\t', 'D\t', 'R\t'])):
      fileName = line.split('/')
      fileName = fileName[len(fileName)-1]
      if(fileName.find('.') == -1):
        continue
      fileName = line.split('.')
      ext = "type_"+fileName[len(fileName)-1]
      if(not ext in exts):
        exts.append(ext.replace('\n',''))
  exts = pd.DataFrame(exts)
  for col in exts[0].unique():
    commitIds[col] = 0

  for index, row in commitIds.iterrows():
    extLines = []
    update = readChangeDetails(row["commit"], currentRepo) 
    for line in update.splitlines():
      if line.startswith(tuple(['A\t', 'M\t', 'D\t', 'R\t'])):
        fileName = line.split('/')
        fileName = fileName[len(fileName)-1]
        if(fileName.find('.') == -1):
          continue
        fileName = line.split('.')
        extName = "type_" + fileName[len(fileName)-1]
        if(not extName in extLines):
          extLines.append(extName)

    ## exts now has all the filetypes hit by the commits so we set them to 1
    for ext in extLines :
      commitIds.at[index,ext]=1

  return commitIds.set_index('commit')

def folders_changed(Ids, text) :
  for line in text.splitlines() :
    if line.startswith(tuple(['A\t', 'M\t', 'D\t', 'R\t'])):
      dirName = line.split('/')[0][2:].replace('\n','')
      # add the marker to appropriate folder
      # print(dirName)
    bow = pd.DataFrame([1],columns=[dirName])
    bow['id'] = Ids
    return bow.set_index('id')

def setIgnoreWordsinText() :
    return ["and", "as", "of", "by", "do", "for", "from", "in", "it", "no", "more", "commit", "function",
              "is", "into", "on", "to", "this", "noe", "the", "that", "with", "then", "we", "when", "use"]

def wordVectors(ids, comments) :
    vectorizer = CountVectorizer(
        max_df=0.95, min_df=2,
        max_features=100,
        stop_words=frozenset(setIgnoreWordsinText()))
    words = vectorizer.fit_transform(comments)
    cols = list(map(lambda w: "w_"+ w, vectorizer.get_feature_names_out()))
    bow = pd.DataFrame(words.toarray(),columns=cols)
    bow['id'] = ids
    return bow.set_index('id')

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  os.chdir(root)
  sourceFile = "{0}.csv".format(oneWayEncrypt(currentRepo))
  if not os.path.exists('{0}{1}'.format(root, sourceFile)):
    print('{0} not found')
    continue
  results = []
  ## read the csv  --- 
  ## loop over the results and Write the output
  df = pd.read_csv(sourceFile, names = ['commit', 'author', 'date', 'comment'])
  df = df.reset_index()
  # build a list of all words 
  wordVect = wordVectors(df["commit"], df["comment"].values.astype('U'))
  topLevelFolders = extractTopLevelFolders(df["commit"])
  toFileTypes = extractFileTypes(df["commit"])

  for index, row in df.iterrows():
    utc_datetime = dt.strptime(row["date"], '%Y-%m-%dT%H:%M:%S%z').astimezone(pytz.utc)
    date = utc_datetime.strftime("%Y-%m-%d")
    year_month = utc_datetime.strftime("%Y-%m")
    hour_of_day = utc_datetime.hour
    day_of_year = utc_datetime.strftime('%j')
    day_of_week = utc_datetime.weekday()
    isWeekend = 0
    if utc_datetime.weekday() > 4:
      isWeekend = 1
    person = oneWayEncrypt(row["author"])
    commitId = row["commit"]
    changeText = readChangeDetails(commitId, currentRepo)
    files_modified = fileCount(changeText, "M")
    files_added = fileCount(changeText, "A")
    files_deleted = fileCount(changeText, "D")
    files_renamed = fileCount(changeText, "R")
    files_changed = files_modified + files_deleted + files_added + files_renamed
    isMerge = isMergeRequest(changeText)
  #-- record the results to the datafile
    results.append([commitId, date, year_month, hour_of_day, day_of_year, day_of_week, files_changed, files_modified, files_added, files_deleted, files_renamed, isWeekend, isMerge, person ])
  results = pd.DataFrame(results, columns = ['id', 'date', 'year_month', 'hour_of_day', 'day_of_year', 'day_of_week', 'files_changed', 'files_modified', 'files_added', 'files_deleted', 'files_renamed', 'isWeekend', 'isMerge', 'person']).set_index('id')
  results = pd.concat([results, topLevelFolders, toFileTypes, wordVect], axis=1)
  results_to_csv = pd.DataFrame(results.fillna(0))
  results_to_csv.to_csv("data_{0}.csv".format(oneWayEncrypt(currentRepo), index=True, index_label="id"))

"""#Testing Sections"""

from hashlib import shake_256
from sklearn.feature_extraction.text import HashingVectorizer 

d = ["John likes to watch movies",
"Mary likes to watch movies this is a very long string",
"Jane makes popcorn"]

# xx = levenshtein(d[0], d[1])
# print(xx)

# def lev_metric(x, y):
#      i, j = int(x[0]), int(y[0])     # extract indices
#      return levenshtein(data[i], d[j])

# X = np.arange(len(d)).reshape(-1, 1)

# hvec = HashingVectorizer(n_features=2**4)
# v = hvec.transform(d).toarray()
# print("d[0]):",np.prod(v[0]))
# print("d[1]):",np.prod(v[1]))
# print("d[2]):",np.prod(v[2]))
# print("similarity(d[0],d[1]):",np.dot(v[0],v[1]))

# print("similarity(d[0],d[1]):",np.dot(v[0],v[1]))
# print("similarity(d[0],d[2]):",np.dot(v[0],v[2]))
# print("similarity(d[1],d[2]):",np.dot(v[1],v[2]))

"""## Clean out repos
clean up the repo as the code does not mapper once the extracts take place.
"""

import shutil
os.chdir(root)

if(clean_up_after_export):
  for allowedRepo in allowedRepos :
    currentRepo = allowedRepo.split('/')[1]
    #clean up the source data
    if os.path.exists('{0}.csv'.format(oneWayEncrypt(currentRepo))):
      os.remove('{0}.csv'.format(oneWayEncrypt(currentRepo)))
    if os.path.exists('{0}_changes.csv'.format(oneWayEncrypt(currentRepo))):
      os.remove('{0}_changes.csv'.format(oneWayEncrypt(currentRepo)))

  for allowedRepo in allowedRepos :
    currentRepo = allowedRepo.split('/')[1]
    if os.path.exists(root + currentRepo):
      shutil.rmtree(root + currentRepo)
  os.listdir()

"""# Graph the results 
Over time we need to evaluate if the project is worth testing
- graph number of commits over time 
- graph commits per person - as above by user
- show scatter charts person/number/month
"""

## display the data headers to ensure factor values are correct 
for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  print(currentRepo)
  data = readSourceDataFile(oneWayEncrypt(currentRepo)).set_index('id')

"""### Check the spread of dates by person to find clusters"""

def scatterPlot(data, fieldName, title, seperatePlots = False):

  commiters = len(data.groupby('person')["person"].count())
  if(seperatePlots):
    fig, axis = plt.subplots(nrows=1, ncols=commiters, figsize=[20,4])
    fig.tight_layout()
    idx=0  
    for name, group in data.groupby('person'):
      size = len(group.groupby(fieldName)) * 10
      axis[idx].scatter(group[fieldName + '_sin'], group[fieldName + '_cos'], marker='o', s=size,label=name[:4])
      axis[idx].legend()
      idx=idx+1
    fig.show()
  else :
    for name, group in data.groupby('person'):
      size = len(group.groupby(fieldName)) * 10
      plt.scatter(group[fieldName + '_sin'], group[fieldName + '_cos'], marker='o', s=size,label=name[:4])
      plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
    plt.title(title)
    plt.show()

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  scatterPlot(data, 'hour_of_day', '{0} Person active by hour'.format(oneWayEncrypt(currentRepo)[:3]))
  scatterPlot(data, 'day_of_week', '{0} Person active by in week'.format(oneWayEncrypt(currentRepo)[:3]))
  scatterPlot(data, 'day_of_year', '{0} Person active in year'.format(oneWayEncrypt(currentRepo)[:3]))

"""### Bar chart results to see overall structure"""

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  data["Name4"] = data["person"].str[:5]
  # -- Barchart
  group = data.groupby(['Name4'])['Name4'].agg(['count'])
  plt.figure(figsize=(15, 15))
  group.plot.bar(y='count', label='Total Check-ins', title="commits by person for {0}".format(oneWayEncrypt(currentRepo)[:3]))
  byDate = data.groupby(['year_month'])['year_month'].count().to_frame()
  # Plot checkin totals
  plt.figure(figsize=(10, 10))
  plt.plot(byDate.cumsum(), color='red')
  plt.title('{0} check-in rete over time'.format(oneWayEncrypt(currentRepo)[:3]), fontsize=14)
  plt.xlabel('date', fontsize=14)
  plt.ylabel('Check in', fontsize=14)
  plt.grid(True)
  plt.show()

"""### Time Series - Trends and Totals
Review the work load for each contributor to see if trends can be identified
"""

f = plt.figure()
f.set_figwidth(7)
f.set_figheight(5)
filter_date = dt.today() - timedelta(weeks=filter_weeks)

## Totals
for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]

  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  print("Checkin Totals")
  print("{0} - {1}".format(oneWayEncrypt(currentRepo)[:3], data['id'].count()))
## Grouped checkins
for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  byDate = data.groupby('date')['date'].count()
  plt.plot(byDate.cumsum(), label=oneWayEncrypt(currentRepo)[:3] )
plt.xlabel('date', fontsize=14)
plt.ylabel('Check in', fontsize=14)
dpi = 72.
plt.legend(loc="upper left")
plt.savefig('check-totals-chart.png', dpi=dpi)
plt.show()

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  data["Name4"] = data["person"].str[:4]
  group = data.groupby(['Name4'])['Name4'].agg(['count'])
  group.plot.bar(y='count', title="commits by person for {0}".format(oneWayEncrypt(currentRepo)[:3]))

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  data = data[(data["date"] >= filter_date)].fillna(0)
  timeseries = data.set_index('date')
  # Select the proper time period for weekly aggreagation
  y = timeseries['files_changed'].resample('W').sum()
  fig, ax = plt.subplots(figsize=(12, 6))
  fig.suptitle("Trend of recent updates {0}".format(oneWayEncrypt(currentRepo)[:3]), fontsize=16)
  ax.plot(y,marker='.', linestyle='-', linewidth=0.5, label='Weekly')
  ax.plot(y.resample('M').mean(),marker='o', markersize=8, linestyle='-', label='Monthly Mean')
  ax.set_ylabel('files_changed')
  ax.legend();

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  data = data[(data["date"] >= filter_date)].fillna(0)
  fig, ax = plt.subplots(figsize=(15, 8))
  fig.suptitle("Trend of recent user updates {0}".format(oneWayEncrypt(currentRepo)[:3]), fontsize=16)
  for name, personData in data.groupby("person") :
    personData = data[data["person"] == name]
    personData = personData.set_index('date')
    # Select the proper time period for weekly aggreagation
    y = personData['files_changed'].resample('D').sum()
    ax.plot(y,marker='.', linestyle='-', linewidth=0.5, label=name[:4])
    ax.plot(y.resample('W').mean(),marker='o', markersize=8, linestyle='-', label='Weekly Mean')
  ax.set_ylabel('files changed')
  ax.legend();

"""# Generate Fraudulant records based on other user checkins renamed as the main user.

This is a generalised function that reads the data file and generates fraud data based on a single user, and taking a set of other users as impersonating the first.  The process attempts to take a balanced number of transactions, however this is not always possible as one develop could have a large number of commits over the period, while the others may less in number.
"""

def GenerateFraud(repoName, 
                  fraud_sample,  
                  filter_weeks, 
                  percentFraud = .25, 
                  data_bow_exclude=False, 
                  data_dir_exclude=False, 
                  data_type_exclude=False, 
                  data_features_exclude = ['id', 'date', 'person'], 
                  data_features_include = ['id', 'date', 'person']) :
# build a training set based on all commits for one person and 25% of commits from the other
# use this a fraud detection for person impersonating another user.
  UserTested = ""
  data = readSourceDataFile(oneWayEncrypt(currentRepo))
  features = data[:]

  # exclude features if specified
  if(data_dir_exclude):
    bow_cols = features.columns[features.columns.str.contains("dir_")]
    features = features.drop(bow_cols,axis=1)

  # exclude features if specified
  if(data_bow_exclude):
    bow_cols = features.columns[features.columns.str.contains("w_")]
    features = features.drop(bow_cols,axis=1)

  # exclude features if specified
  if(data_type_exclude):
    bow_cols = features.columns[features.columns.str.contains("type_")]
    features = features.drop(bow_cols,axis=1)

  grouped = features.groupby('person')["person"] \
                    .count()  \
                    .sort_values(ascending = False)

  print(grouped)
  first = True
  filter_date = dt.today() - timedelta(weeks=filter_weeks)
  for person in grouped.index.tolist():
      if(first):
        # take all these as valid the filter period
        valid_train = features[(features['person']==person) & (features.date < filter_date)].fillna(0).drop(data_features_exclude,axis=1)
        valid_train["valid"] = 0        
        print("valid train = {0}".format(len(valid_train)))
        valid_test = features[(features['person']==person) & (features.date >= filter_date)].fillna(0).drop(data_features_exclude,axis=1)
        valid_test["valid"] = 0
        first=False
        # create temp dataframes with same structure
        fraud_train = valid_test[0:0]
        fraud_test = valid_test[0:0]
        UserTested = person
      else:
        # take a random % of others as impersonated commits
        fraud_tr = features[(features['person']==person) & (features.date < filter_date)].fillna(0).drop(data_features_exclude,axis=1)
        fraud_tr["valid"] = 1
        print("fraud train = {0}".format(len(fraud_tr)))
        fraud_te = features[(features['person']==person) & (features.date >= filter_date)].fillna(0).drop(data_features_exclude,axis=1)
        mask = np.random.choice([False, True], len(fraud_te), p=fraud_sample)
        fraud_te = fraud_te[mask]
        fraud_te["valid"] = 1
        #don't let the test set get to big
        if(len(fraud_test) > (len(valid_test) * percentFraud)):
          fraud_test = fraud_test.truncate(after=round(len(valid_test) * percentFraud))
        fraud_test = pd.concat([fraud_te, fraud_test], ignore_index=True)
        fraud_train = pd.concat([fraud_tr, fraud_train], ignore_index=True)

  #Balance the fraud and valid transactions in the training set
  if(len(fraud_test) > len(valid_test)):
    fraud_test = fraud_test.truncate(after=len(valid_test))  
  if(len(fraud_train) > len(valid_train)):
    fraud_train = fraud_train.truncate(after=len(valid_train))  

  #gather the training sets together
  training_set = valid_train.append(fraud_train, ignore_index=True)
  test_set = valid_test.append(fraud_test, ignore_index=True)

  if(shuffle_data):
    training_set = training_set.sample(frac=1)
    test_set = test_set.sample(frac=1)

#-- breakout if nothing to test or train
  if len(training_set) == 0:
    return;

  #split for return to processor
  training_labels = training_set["valid"]
  training_set = training_set.drop(["valid"],axis=1)
  test_label = test_set["valid"]
  test_set = test_set.drop(["valid"],axis=1)

  return training_set, training_labels, test_set, test_label, UserTested

"""### Chart results of Fraud generation
Creates a simple bar chart based on the two sets of data
"""

import matplotlib.pyplot as plt 
def chartFraud(training, test, title) :
  width =0.3
  # Training set 
  train = pd.DataFrame(training).value_counts()
  plt.bar(np.arange(len(train)), train, width=width, color = 'red')
  plt.xlabel("Training set {0}-Valid/Fraud".format(len(train)))
  plt.title("Training Records for {0}".format(title))
  plt.show()

  # Test set
  test = pd.DataFrame(test).value_counts()
  plt.xlabel("Test set {0}-Valid/Fraud".format(len(test)))
  plt.title("Test Records for {0}".format(title))
  plt.bar(np.arange(len(test)), test, width=width, color = 'blue')
  plt.show()

"""# Supervised Learning
This version matches the credit card testing where we set some of the values to be considered fraud; i.e. a random set of commits for another author is renamed as the previous author
"""

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

def chart_results(predicted, actual, title) :
  d={0:"valid",1:"fraud"}
  predicted_labeled = pd.Series(predicted).apply(lambda x:d[x])
  actual_labeled = pd.Series(actual).apply(lambda x:d[x])

  # print("labeled counts")
  # print(predicted_labeled.value_counts())
  # print(actual_labeled.value_counts())

  cm = confusion_matrix(actual_labeled, pd.Series(predicted_labeled), labels=["valid", "fraud"])
  df_cm = pd.DataFrame(cm, index=["valid", "fraud"], columns=["valid", "fraud"])
  fig = plt.figure(figsize=(7, 7))
  sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')
  plt.setp(plt.gca().get_xticklabels(), ha="right", rotation=45)
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')
  plt.title("Confusion Matrix - {0}".format(title))
  plt.show()
  print("Matrix Scores")
  print("----------------------")
  print("title:",title)
  print("Accuracy:",metrics.accuracy_score(test_label, predicted))
  print("Precision:",metrics.precision_score(test_label, predicted))
  print("Recall:",metrics.recall_score(test_label, predicted))
  print("f1: ", metrics.f1_score(test_label, predicted))
  print("area under curve (auc): ", metrics.roc_auc_score(test_label, predicted))
  print("----------------------")

"""## XGBoost
See.. https://www.datacamp.com/community/tutorials/xgboost-in-python
Also
https://practicaldatascience.co.uk/machine-learning/how-to-create-a-classification-model-using-xgboost
"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# build a training set based on all commits for one person and 20% of commits from the other
# use this a fraud detection for person impersonating another user.

percentFraud = .5
for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  training_set, training_labels, test_set, test_label, TestUser = GenerateFraud(
                                                                      oneWayEncrypt(currentRepo), 
                                                                      fraud_sample,
                                                                      filter_weeks,
                                                                      percentFraud = percentFraud,
                                                                      data_bow_exclude=False, 
                                                                      data_dir_exclude=False, 
                                                                      data_type_exclude=False,                                                                       
                                                                      data_features_exclude = [
                                                                        'id', 'date', 'person',
                                                                        'year_month', 'hour_of_day', 
                                                                        'day_of_year', 'day_of_week'                                                                                              
                                                                      ])
  ## graph to ensure we have a resonable spread 
  chartFraud(training_labels, test_label, currentRepo)
  xg_classification = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
  xgb.XGBClassifier(base_score=0.4, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=2, colsample_bytree=5, eval_metric='mlogloss',
              gamma=0, gpu_id=-1, importance_type='gain',
              interaction_constraints='', learning_rate=0.2000,
              max_delta_step=0, max_depth=4, min_child_weight=1,
              monotone_constraints='()', n_estimators=100, n_jobs=16,
              num_parallel_tree=1, objective='multi:softprob', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,
              tree_method='exact', use_label_encoder=False,
              validate_parameters=3, verbosity=None)

  X_train, X_test, y_train, y_test = train_test_split(training_set, training_labels, test_size=0.2, random_state=123)
  xg_classification.fit(X_train, y_train)
  predicted = xg_classification.predict(test_set)

  print("predicted: {0} - {1}".format(pd.Series(predicted).array, len(predicted)))
  print("test set:  v{0} - f{1}".format(len(test_label[test_label == 0]), len(test_label[test_label > 0])))
  ## show the results
  chart_results(predicted, test_label, oneWayEncrypt(currentRepo)[:3])

"""## KNeighborsClassifier"""

from sklearn.neighbors import KNeighborsClassifier
from pandas._libs.algos import groupsort_indexer

# build a training set based on all commits for one person and 20% of commits from the other
# use this a fraud detection for person impersonating another user.

for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  training_set, training_labels, test_set, test_label, TestUser = GenerateFraud(
                                                                      oneWayEncrypt(currentRepo), 
                                                                      fraud_sample,
                                                                      filter_weeks,
                                                                      data_bow_exclude=False, 
                                                                      data_dir_exclude=False, 
                                                                      data_type_exclude=False,  
                                                                      data_features_exclude = [
                                                                        'id', 'date', 'person',
                                                                        'year_month', 'hour_of_day', 
                                                                        'day_of_year', 'day_of_week'                                                                                              
                                                                      ])
  ## graph to ensure we have a resonable spread 
  chartFraud(training_labels, test_label, currentRepo)

  knn = KNeighborsClassifier()
  knn.fit(training_set, training_labels)
  predicted = knn.predict(test_set)
  print("predicted: {0} - {1}".format(pd.Series(predicted).array, len(predicted)))
  print("test set:  v{0} - f{1}".format(len(test_label[test_label == 0]), len(test_label[test_label > 0])))
  ## show the results
  chart_results(predicted, test_label, oneWayEncrypt(currentRepo)[:3])

"""## Support Vector Machine

"""

from sklearn import svm, metrics


for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  training_set, training_labels, test_set, test_label, TestUser = GenerateFraud(oneWayEncrypt(currentRepo), 
                                                                      fraud_sample,
                                                                      filter_weeks,
                                                                      data_bow_exclude=False, 
                                                                      data_dir_exclude=False, 
                                                                      data_type_exclude=False,  
                                                                      data_features_exclude = [
                                                                        'id', 'date', 'person',
                                                                        'year_month', 'hour_of_day', 
                                                                        'day_of_year', 'day_of_week'                                                                                              
                                                                      ])
  ## graph to ensure we have a resonable spread 
  chartFraud(training_labels, test_label, currentRepo)

  clf = svm.SVC(gamma='auto')
  clf.fit(training_set, training_labels)
  predicted = clf.predict(test_set)

  print("predicted: {0} - {1}".format(pd.Series(predicted).array, len(predicted)))
  print("test set:  v{0} - f{1}".format(len(test_label[test_label == 0]), len(test_label[test_label > 0])))
  ## show the results
  chart_results(predicted, test_label, oneWayEncrypt(currentRepo)[:3])

"""## DecisionTreeClassifier

"""

from sklearn import tree, metrics


for allowedRepo in allowedRepos :
  currentRepo = allowedRepo.split('/')[1]
  training_set, training_labels, test_set, test_label, TestUser = GenerateFraud(oneWayEncrypt(currentRepo), 
                                                                      fraud_sample,
                                                                      filter_weeks,
                                                                      data_features_exclude = [
                                                                        'id', 'date', 'person',
                                                                        'year_month', 'hour_of_day', 
                                                                        'day_of_year', 'day_of_week'                                                                                              
                                                                      ])
  ## graph to ensure we have a resonable spread 
  chartFraud(training_labels, test_label, currentRepo)

  clf = tree.DecisionTreeClassifier()
  clf.fit(training_set, training_labels)
  predicted = clf.predict(test_set)

  print("predicted: {0} - {1}".format(pd.Series(predicted).array, len(predicted)))
  print("test set:  v{0} - f{1}".format(len(test_label[test_label == 0]), len(test_label[test_label > 0])))
  ## show the results
  chart_results(predicted, test_label, oneWayEncrypt(currentRepo)[:3])

"""# Unsupervised Learning

This version matches the money laundering testing where there is no confirmed values that can be used and we deal with outlier detection.

## One Class Support Vector Machine Approach

https://www.activestate.com/resources/quick-reads/how-to-classify-data-in-python/
"""

def chartSVM_results(results) :
  results['anomaly'] = results['y_Predicted'].apply(lambda x: 'outlier' if x==-1 else 'inlier' )
  results.pivot(columns='anomaly', values='score').plot.hist(title='One Class SMV Outliers {0}'.format(person))
  mask = results['y_Predicted'] == -1
  if(len(mask) == 0):
    print("No outliers found for {0}".format(currentRepo))
    return
  if(len(results[mask]) == 0):
    return
  print(results[mask][['id', 'date', 'person','score']])

import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, class_names, normalize=None,
                          title='Confusion Matrix', plot_numbers=False, display_names=None,
                          figsize=(15, 11)):
    cm = confusion_matrix(y_true, y_pred, normalize=normalize)
    if not display_names:
        display_names = class_names
    df_cm = pd.DataFrame(cm, index=display_names, columns=display_names)
    fig = plt.figure(figsize=figsize)
    sns.heatmap(df_cm, annot=plot_numbers, cmap='Blues', fmt='g')
    plt.setp(plt.gca().get_xticklabels(), ha="right", rotation=45)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    # plt.title(title)
    return fig

from pandas.core import missing
from sklearn.svm import OneClassSVM
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from numpy import quantile, where, random


for allowedRepo in allowedRepos :
    training_set, training_labels, test_set, test_label, TestUser = GenerateFraud(oneWayEncrypt(currentRepo), 
                                                                      fraud_sample,
                                                                      filter_weeks,
                                                                      data_bow_exclude=True,
                                                                      data_dir_exclude=False,
                                                                      data_type_exclude=False,
                                                                      data_features_exclude = [
                                                                        'id', 'date', 'person',
                                                                        'year_month', 'hour_of_day', 
                                                                        'day_of_year', 'day_of_week'                                                                                              
                                                                      ])
    ## graph to ensure we have a resonable spread 
    chartFraud(training_labels, test_label, currentRepo)

    #--  Fit the One-Class SVM  --
    # training_set get only valid items then test against the test set
    mask = training_labels[:] == 0
    training_set = training_set[mask]
    svm = OneClassSVM(gamma='auto', nu=0.1, kernel="rbf").fit(training_set)
    predicted = svm.predict(test_set)
    predicted = pd.Series(predicted).apply(lambda x: 1 if x==-1 else 0 )
    print("predicted: {0} - {1}".format(pd.Series(predicted).array, len(predicted)))
    print("test set:  v{0} - f{1}".format(len(test_label[test_label == 0]), len(test_label[test_label > 0])))
    chart_results(predicted, test_label, oneWayEncrypt(currentRepo)[:3])

"""## Isolation Forect Approach
Based on date vectors, aims to find with items are not within the standard areas for a specific user.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn.metrics import roc_curve
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt

    # "Empirical Covariance": EllipticEnvelope(support_fraction=1.0, contamination=0.25),
    # "Robust Covariance (Minimum Covariance Determinant)": EllipticEnvelope(
    #     contamination=0.25

# %timeit
for allowedRepo in allowedRepos :
    training_set, training_labels, test_set, test_label, TestUser = GenerateFraud(oneWayEncrypt(currentRepo), 
                                                                      fraud_sample,
                                                                      filter_weeks,
                                                                      data_bow_exclude=False,
                                                                      data_dir_exclude=False,
                                                                      data_type_exclude=False,
                                                                      data_features_exclude = [
                                                                        'id', 'date', 'person',
                                                                        'year_month', 'hour_of_day', 
                                                                        'day_of_year', 'day_of_week'                                                                                              
                                                                      ])
    # training_set get only valid items then test against the test set
    mask = training_labels[:] == 0
    training_set = training_set[mask]
    training_set = training_set[['hour_of_day_sin','hour_of_day_cos', 'day_of_week_sin', 'day_of_week_cos']].to_numpy() #  day_of_week_sin  day_of_week_cos']

    iforest = IsolationForest(n_estimators=200, 
                            max_samples='auto', 
                            contamination=0.20, 
                            bootstrap=False, 
                            random_state=1)
    iforest.fit(training_set)
    
    # -- Test
    test_set = test_set[['hour_of_day_sin','hour_of_day_cos', 'day_of_week_sin', 'day_of_week_cos']] #  day_of_week_sin  day_of_week_cos']
    predicted = iforest.predict(test_set.to_numpy())
    predicted = pd.Series(predicted).apply(lambda x: 1 if x==-1 else 0 )
    print("predicted: {0} - {1}".format(pd.Series(predicted).array, len(predicted)))
    print("test set:  v{0} - f{1}".format(len(test_label[test_label == 0]), len(test_label[test_label > 0])))
    chart_results(predicted, test_label, oneWayEncrypt(currentRepo)[:3])